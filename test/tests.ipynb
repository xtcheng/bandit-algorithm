{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d3845c-a0ea-401e-9056-b8331ca9b406",
   "metadata": {},
   "source": [
    "This notebook has been created to streamline all the tests we have definined for our implementations, be it to assert their intended funcionality or to challenge them against new situations. This document is divided into various sections, roughly following the logic of collecting existing basics to eventually recombine them to something original. Each test will have a brief explanation and a discussion of any results that are worth noting.\n",
    "\n",
    "Note that this document contains everything required to interface with our full corpus of code. Feel free to modify things to make it perform different tasks.\n",
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892920c",
   "metadata": {},
   "source": [
    "To run these tests, execute the following lines of code once. Then, each test can be run standalone. Unless noted otherwise, all algorithms are implementations of existing research which is referenced in the readme.\n",
    "\n",
    "First, a few imports we will need in most tests. The point of reference is the root folder of this project, which is one step above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import helpers.masterTester as tester\n",
    "from environment.Gaussian_noise import Gaussian_noise as gn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915cc231-a5b6-4c91-a82a-2e9202a19259",
   "metadata": {},
   "source": [
    "The testcode will refer to some global variables that we can set her, outside the original file. (If not set, they will be defaulted to some meaningful values.)\n",
    "\n",
    "For best performance, set AVAILABLE_CORES to the number of cores on your machine or lower. High values may cause excessive memory usage and crashes. Set quiet to False to see information on process assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c7ff14-f69f-4b15-ab93-f5626b3de3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.ERRORBAR_COUNT = 40\n",
    "tester.AVAILABLE_CORES = 8\n",
    "tester.QUIET = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d160a1b-7069-44d5-8e30-22c80edcb80e",
   "metadata": {},
   "source": [
    "Generally, our results are written into files, then read in again and plotted. The test function streamlines this process, but you can still call the internal functions directly, for example to plot something again without recalculating it, or to add more subtests to existing data, which will then all be plotted together. Just be aware that the test function will delete previous data on default before calling testOnly, readAllResults and plotResults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1190440-7c61-46e7-8ec7-8feefaa87d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.readAllResults()\n",
    "tester.plotResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d2522b-679b-46ec-b779-024cf4b7672d",
   "metadata": {},
   "source": [
    "**Important:** Some of the tests will take very long to execute on an average system. When running them locally, you should put the shell from which you launched jupyter notebook into focus. Otherwise (at least in Windows 11), the scheduler of you OS will assign less processing power to the test, making it take even longer. If you are still not satisfied, you can try to copy the code above and the test code into a .py file and execute it normally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5b154-a2ce-41b1-b3f7-f4133a25d019",
   "metadata": {},
   "source": [
    "# Stationary, Single Objective\n",
    "These cases will test the implementations of strategies that operate on the standard MAB-model. Each arm has a fixed expectation value unknown to the agent and will return this value plus random noise as feedback when chosen. The agent will accumulate regret whenever it does not choose the best arm, so it needs to find it quickly while being sufficiently certain that no other arm is better on average.\n",
    "\n",
    "Note that many of these tests are from an early phase of our research, which is why they do not make use of the streamlined interfaces that we utilize later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d36c52-59bf-4e48-ac86-c0c7efe31500",
   "metadata": {},
   "source": [
    "## Test 1\n",
    "This first test will run the well known strategies. These include different flavours of UCB, which works by estimating by how much an arm's average might still improve as long as there is insufficient data on that arm and adds that onto the average when choosing the current best arm. The other strategy is Epsilon Greedy, which simply picks either the arm with the best raw average or some random arm, but gradually reduces the amount of random choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf5be29-02c1-4057-8482-8bba698d6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri May 31 11:17:30 2024\n",
    "\n",
    "@author: Xiaotong\n",
    "\n",
    "test the algorithms in paper \"Finite-time Analysis of the Multiarmed Bandit Problem\"\n",
    "\"\"\"\n",
    "\n",
    "from environment.Gaussian_noise import Gaussian_noise as gn\n",
    "from environment.env import env_stochastic\n",
    "from algorithms.UCB2 import UCB2\n",
    "from algorithms.epsilion import EpsilonGreedy\n",
    "from algorithms.UCB1 import UCB1\n",
    "from algorithms.UCB1N import UCB1N \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_arm = 5\n",
    "T = 100\n",
    "\n",
    "mu = np.array([0.1, 0.9, 0.4, 0.3, 0.6])\n",
    "Trial = 10\n",
    "noise = gn(1,0,0.01,[-0.1,0.1])\n",
    "env = env_stochastic(num_arm, mu, noise)\n",
    "\n",
    "\n",
    "algorithms = [UCB2(T, num_arm, alpha=0.1), EpsilonGreedy(T, num_arm, c=0.01, d=0.05), UCB1(T, num_arm), UCB1N(T, num_arm)]\n",
    "algorithm_names = ['UCB2', 'Epsilon-Greedy', 'UCB1', 'UCB1N']\n",
    "avg_regret = []\n",
    "\n",
    "for i, algorithm in enumerate(algorithms):\n",
    "    regret_sum = 0\n",
    "    for trial in range(Trial):\n",
    "        algorithm.clear()\n",
    "        algorithm.run(env)\n",
    "        regret_sum += algorithm.get_cum_rgt()[-1]\n",
    "    avg_regret.append(regret_sum / Trial)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "for i, algorithm in enumerate(algorithms):\n",
    "    plt.plot(range(T), algorithm.get_cum_rgt(), label=algorithm_names[i])\n",
    "plt.xlabel('t (Trials)', fontsize=15)\n",
    "plt.ylabel('Cumulative Regret', fontsize=15)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Cumulative Regret')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "for i, algorithm in enumerate(algorithms):\n",
    "    plt.plot(range(T), algorithm.get_avg_rgt(), label=algorithm_names[i])\n",
    "plt.xlabel('t (Trials)', fontsize=15)\n",
    "plt.ylabel('Average Regret', fontsize=15)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Average Regret')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc0524-40ee-4f6a-9ce3-15f75b8dd402",
   "metadata": {},
   "source": [
    "Except for our UCB1N implementation, which shows inconsistent behaviour for at least 200 timesteps, all implementations achieve low regret in a short time, within only 100 timesteps. Notably though, UCB1 has trouble converging whithin this frame. UCB2 and Epsilon-Greedy on the other hand manage to almost exclusively pick the best arm only towards the end of the test. Interestingly, while UCB2 is an even more sophisticated version of UCB1, Epsilon-Greedy reaches the same or better level of performance in practise with a much more simplistic approach.\n",
    "\n",
    "## Test 2\n",
    "The second test will be conducted for the cooperative UCB strategies. Here, multiple agents will interact with the environment themselves, but share information on the findings with each other. However, not all agents are necessarily pairwise connected. For example, in our test, agent 1 can communicate with agents 2 and 5 (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af3725-6514-4263-b739-2ae2605c18ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri May 31 11:42:42 2024\n",
    "\n",
    "@author: Xiaotong\n",
    "\n",
    "test the algorithms in paper \"Distributed cooperative decision making in multi-agent multi-armed bandits\"\n",
    "\"\"\"\n",
    "\n",
    "# main.py\n",
    "import matplotlib.pyplot as plt\n",
    "from environment.env import env_stochastic\n",
    "\n",
    "from algorithms.CoopUCB2sl import CoopUCB2Selective\n",
    "from algorithms.CoopUCB2 import CoopUCB2\n",
    "\n",
    "# Parameters\n",
    "T = 5000  # Number of trials\n",
    "num_agents = 5\n",
    "num_arms = 10\n",
    "mu = [0.4, 0.5, 0.5, 0.6, 0.7, 0.7, 0.8, 0.9, 0.92, 0.95]\n",
    "observation_matrix = [\n",
    "    [0, 1, 0, 0, 1],\n",
    "    [1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 1, 0 ],\n",
    "]\n",
    "gamma = 1\n",
    "eta = 0.5\n",
    "sigma_g = 2.5\n",
    "# Initialize environment and algorithm\n",
    "\n",
    "noise = gn(1,0,0.01,[-0.1,0.1])\n",
    "env = env_stochastic(num_arms, mu, noise)\n",
    "\n",
    "algorithm = CoopUCB2(T, num_agents, num_arms, observation_matrix, gamma, sigma_g, eta)\n",
    "algorithm2 = CoopUCB2Selective(T, num_agents, num_arms, observation_matrix, gamma, sigma_g, eta)\n",
    "\n",
    "# Run the algorithm\n",
    "algorithm.run(env)\n",
    "algorithm2.run(env)\n",
    "# Get average regret\n",
    "avg_regret = algorithm.get_avg_rgt()\n",
    "cum_regret = algorithm.get_cum_rgt()\n",
    "\n",
    "avg_regret2 = algorithm2.get_avg_rgt()\n",
    "cum_regret2 = algorithm2.get_cum_rgt()\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.plot(avg_regret)\n",
    "plt.xlabel('Trials')\n",
    "plt.ylabel('Average Regret')\n",
    "plt.title('Performance of coop_ucb Algorithm')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting\n",
    "plt.plot(cum_regret)\n",
    "plt.xlabel('Trials')\n",
    "plt.ylabel('Cummulative Regret')\n",
    "plt.title('Performance of coop_ucb Algorithm')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots( 2,figsize=(12, 5))\n",
    "# Plot cumulative regret for each agent for Case 1\n",
    "for agent in range(num_agents):\n",
    "    axs[0].plot(algorithm.cumulative_regret[agent], label=f'Agent {agent+1}')\n",
    "axs[0].set_title('Cumulative Regret of coopUCB Algorithm')\n",
    "axs[0].set_xlabel('Trials')\n",
    "axs[0].set_ylabel('Cumulative Regret')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "for agent in range(num_agents):\n",
    "    axs[1].plot(algorithm.average_regret[agent], label=f'Agent {agent+1}')\n",
    "axs[1].set_title('Cumulative Regret of coopUCB Algorithm')\n",
    "axs[1].set_xlabel('Trials')\n",
    "axs[1].set_ylabel('Average Regret')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting\n",
    "plt.plot(avg_regret2)\n",
    "plt.xlabel('Trials')\n",
    "plt.ylabel('Average Regret')\n",
    "plt.title('Performance of coop_ucb Algorithm')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting\n",
    "plt.plot(cum_regret2)\n",
    "plt.xlabel('Trials')\n",
    "plt.ylabel('Cummulative Regret')\n",
    "plt.title('Performance of coop_ucb Algorithm')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots( 2,figsize=(12, 5))\n",
    "# Plot cumulative regret for each agent for Case 1\n",
    "for agent in range(num_agents):\n",
    "    axs[0].plot(algorithm2.cumulative_regret[agent], label=f'Agent {agent+1}')\n",
    "axs[0].set_title('Cumulative Regret of coopUCB Algorithm')\n",
    "axs[0].set_xlabel('Trials')\n",
    "axs[0].set_ylabel('Cumulative Regret')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "for agent in range(num_agents):\n",
    "    axs[1].plot(algorithm2.average_regret[agent], label=f'Agent {agent+1}')\n",
    "axs[1].set_title('Average Regret of coopUCB Algorithm')\n",
    "axs[1].set_xlabel('Trials')\n",
    "axs[1].set_ylabel('Average Regret')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1847f-3ef9-4715-b6c1-3463108ff177",
   "metadata": {},
   "source": [
    "## Test 3\n",
    "This case will test our implementation of the MAMAB_SI strategy. Similar to before, it employs multiple UCB-agents, but instead of directly sharing estimates with each other, each agent only has an individual chance of observing its neighbours at a given time and only learns about this instantaneous result then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa5c47-cde1-4060-a6c5-ec16e45e027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri May 31 11:51:09 2024\n",
    "test algorithm in \"Heterogeneous Stochastic Interactions for Multiple Agents in a Multi-armed Bandit Problem\"\n",
    "@author: Xiaotong\n",
    "\"\"\"\n",
    "\n",
    "# main.py\n",
    "import matplotlib.pyplot as plt\n",
    "from environment.env import env_stochastic\n",
    "\n",
    "from algorithms.MAMAB_SI import MAMAB_SI\n",
    "\n",
    "# Parameters\n",
    "T = 50000  # Number of trials\n",
    "num_agents = 6\n",
    "num_arms = 10\n",
    "mu = [0.4, 0.5, 0.5, 0.6, 0.7, 0.7, 0.8, 0.9, 0.92, 0.95]\n",
    "sociabilities = [0.50, 0.85, 0.05, 0.50, 1.00, 0.90]\n",
    "observation_matrix = [\n",
    "    [0, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 1, 1, 1, 1],\n",
    "    [1, 1, 0, 1, 1, 1],\n",
    "    [1, 1, 1, 0, 1, 1],\n",
    "    [1, 1, 1, 1, 0, 1],\n",
    "    [1, 1, 1, 1, 1, 0]\n",
    "]\n",
    "\n",
    "# Initialize environment and algorithm\n",
    "noise = gn(1,0,0.01,[-0.1,0.1])\n",
    "env = env_stochastic(num_arms, mu, noise)\n",
    "algorithm = MAMAB_SI(T, num_agents, num_arms, sociabilities, observation_matrix)\n",
    "\n",
    "# Run the algorithm\n",
    "algorithm.run(env)\n",
    "\n",
    "# Get average regret\n",
    "avg_regret = algorithm.get_avg_rgt()\n",
    "cum_regret = algorithm.get_cum_rgt()\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.plot(avg_regret)\n",
    "plt.xlabel('Trials')\n",
    "plt.ylabel('Average Regret')\n",
    "plt.title('Performance of MAMAB_SI Algorithm')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting\n",
    "plt.plot(cum_regret)\n",
    "plt.xlabel('Trials')\n",
    "plt.ylabel('Average Regret')\n",
    "plt.title('Performance of MAMAB_SI Algorithm')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb8d1c-1606-4c9f-aaa5-426c388f3a89",
   "metadata": {},
   "source": [
    "We did not test this strategy further than that, so other researchers may find it interesting to look more into the details. For example, the original paper predicts that an agent is most likely to perform well if is has a high sociability, but is connected to agents with a low one, so it profits from the exploration others do and these others actually do this exploration because they hardly look at others' findings themselves.\n",
    "\n",
    "## Test 4\n",
    "This case tests yet another multi agent flavour of UCB, the MUCB strategy. As before, it was considered as standalone and not picked up by us again later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63600d-9822-4482-a8cc-f9adcdba7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri May 31 12:00:06 2024\n",
    "\n",
    "test algorithm in \"Combinatorial Network Optimization With Unknown Variables: Multi-Armed BanditsWith Linear Rewards and Individual Observations\"\n",
    "\n",
    "@author: Xiaotong\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from algorithms.mucb import MUCB\n",
    "from environment.env import env_stochastic\n",
    "\n",
    "num_agents = 4\n",
    "num_arms = 6\n",
    "num_trials = 1000\n",
    "ucb1 = MUCB(num_agents, num_trials, num_arms)\n",
    "mu = [0.1, 0.2, 0.3, 0.4, 0.5, 0.8]  \n",
    "\n",
    "noise = gn(1,0,0.01,[-0.1,0.1])\n",
    "env = env_stochastic(num_arms, mu, noise)\n",
    "\n",
    "num_simulations = 1\n",
    "\n",
    "avg_cumulative_regret = np.zeros(num_trials)\n",
    "avg_average_regret = np.zeros(num_trials)\n",
    "\n",
    "for _ in range(num_simulations):\n",
    "    ucb1 = MUCB(num_agents, num_trials, num_arms)\n",
    "    ucb1.run(env)\n",
    "    avg_cumulative_regret += ucb1.get_cum_rgt()\n",
    "    avg_average_regret += ucb1.get_avg_rgt()\n",
    "    ucb1.clear()\n",
    "\n",
    "\n",
    "avg_cumulative_regret /= num_simulations\n",
    "avg_average_regret /= num_simulations\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_trials), avg_average_regret)\n",
    "plt.title('Average Regret Over Time')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Average Regret')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_trials), avg_cumulative_regret)\n",
    "plt.title('Cumulative Regret Over Time')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Cumulative Regret')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4f385-6eaf-4556-9269-203de2bd655d",
   "metadata": {},
   "source": [
    "## Test Modular\n",
    "This test serves the sole purpose of asserting that our modular implementation of UCB1 does the same as our previous one. We starting modularizing the strategies to avoid redundances in code, partly by utilizing inheritance, and to be able to freely combine strategies with different breakpoint adaption mechanics that only had a specific strategy in mind (usually UCB with forced exploration) in their original papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f63d4-5996-4257-9c74-1719e5616f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.env import env_stochastic\n",
    "from algorithms.UCB1 import UCB1\n",
    "from algorithms.modular.moduleUsers.ucb import ModularUCB\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tnum_arm = 5\n",
    "\tT = 1000\n",
    "\n",
    "\t# Stochastic\n",
    "\tmu = np.array([0.1, 0.9, 0.4, 0.3, 0.6])\n",
    "\tTrial = 10\n",
    "\tnoise = gn(1,0,0.01,[-0.1,0.1])\n",
    "\tenvs = list()\n",
    "\tenvs.append(env_stochastic(num_arm, mu, noise))\n",
    "\n",
    "\n",
    "\talgorithms = [UCB1(T, num_arm,2), ModularUCB(T, num_arm,2)]\n",
    "\talgorithm_names = ['Original UCB', 'Modular implementation']\n",
    "\tenv_names = ['stochastic']\n",
    "\n",
    "\ttester.test(T, Trial, envs, algorithms, algorithm_names, env_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d656b0-5aa1-4db0-ab32-edce9801e24f",
   "metadata": {},
   "source": [
    "# Miscellaneous\n",
    "## Test 5\n",
    "This tests will match UCB1 and exp3 against the usual environment from before, but also against three different adversary environments that would generally allow no assumptions on their nature and may even be designed to confuse specific strategies. A strategy specifically tailored for this scenario is the exp3-strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdae5e8-0509-496e-93f0-ce6c23701518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri May 31 11:17:30 2024\n",
    "\n",
    "@author: Xiaotong\n",
    "\n",
    "test the algorithms in paper \"Finite-time Analysis of the Multiarmed Bandit Problem\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from environment.env import env_stochastic\n",
    "from environment.envAd1 import env_adverse1\n",
    "from environment.envAd2 import env_adverse2\n",
    "from environment.envAd3 import env_adverse3\n",
    "from algorithms.exp3 import exp3\n",
    "from algorithms.epsilion import EpsilonGreedy\n",
    "from algorithms.UCB1 import UCB1\n",
    "\n",
    "num_arm = 5\n",
    "T = 1000\n",
    "\n",
    "# Stochastic\n",
    "mu = np.array([0.1, 0.9, 0.4, 0.3, 0.6])\n",
    "Trial = 10\n",
    "noise = gn(1,0,0.01,[-0.1,0.1])\n",
    "envs = list()\n",
    "envs.append(env_stochastic(num_arm, mu, noise))\n",
    "envs.append(env_adverse1(num_arm, noise))\n",
    "envs.append(env_adverse2(num_arm, noise))\n",
    "envs.append(env_adverse3(num_arm, noise, difficulty=100))\n",
    "\n",
    "\n",
    "algorithms = [exp3(T, num_arm, alpha=0.1, gamma=0.3), UCB1(T, num_arm)]\n",
    "algorithm_names = ['exp3', 'UCB1']\n",
    "env_names = ['stochastic', 'biggerBetter', 'random', 'shifting']\n",
    "\n",
    "tester.test(T, Trial, envs, algorithms, algorithm_names, env_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7148c1-0f6e-4ba2-b310-87924ae5c604",
   "metadata": {},
   "source": [
    "As can be expected, UCB1 perform best on the normal, stochastic environment. exp3 is not much worse here, but stays with a good linear regret instead of converging further.\n",
    "\n",
    "The other settings have failed to highlight the specific advantages of exp3. The environment where arms with higher indices yield better rewards is basically some kind of stochastic environment too, which is why UCB1 has the edge here as well. The environment where the best arm and its vicinity shift around on regular intervals is closer to the non stationary settings in the next section, for which exp3 seems better suited too. As for the random environment, arguably any strategy is optimal because the best arm is decided at random for each timestep, so if there was a strategy that picked the best arm in more than 1/K cases on average, it would be able to predict the random draws that we consider to be truly random.\n",
    "\n",
    "## Test Mult\n",
    "This test uses a stochastic environment, but with the MAB where the agent has to choose some subset of arms with a fixed size greater than 1 instead of just one arm. (Here: 3 out of 10 arms each timestep.) These have to different arms and the reward that the agent has to optimize is their combined feedback. We did the test in order to test our implementation of exp3M, a strategy that is suited for such a setting. Like the normal exp3, it assigns weights to the arms, but then extracts multiple (here 3) choices from that instead of picking one arm with the derived probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cae3d8-f1ab-4d5c-ba1d-6bd583f17d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.env import env_stochastic\n",
    "from algorithms.exp3M import exp3M\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tnum_arm = 10\n",
    "\tbatch_size = 3\n",
    "\tT = 1000\n",
    "\n",
    "\t# Stochastic\n",
    "\tmu = np.array([0.1, 0.9, 0.4, 0.3, 0.6, 0.1, 0.001, 0.95, 0.7, 0.6])\n",
    "\tTrial = 10\n",
    "\tnoise = gn(1,0,0.01,[-0.1,0.1])\n",
    "\tenvs = list()\n",
    "\tenvs.append(env_stochastic(num_arm, mu, noise))\n",
    "\n",
    "\n",
    "\talgorithms = [exp3M(T, num_arm, batch_size, gamma=0.1)]\n",
    "\talgorithm_names = ['exp3M']\n",
    "\tenv_names = ['stochastic']\n",
    "    #env_name.append('biggerBetter')\n",
    "    #env_name.append('random')\n",
    "    #env_name.append('shifting')\n",
    "\n",
    "\ttester.test(T, Trial, envs, algorithms, algorithm_names, env_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae0fdc-3b85-4032-902e-304123b2d671",
   "metadata": {},
   "source": [
    "What may look like a run of the usual exp3 now actually needs to quickly find and exploit the 3 best arms instead of the single best arm. As can be seen, with sufficient success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b49da21-ca49-4214-ac11-da4e8cce6aab",
   "metadata": {},
   "source": [
    "## Test Contextual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b269761d-b75d-4e08-b5f2-859677669ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.envContextual import EnvContextual\n",
    "from algorithms.UCB1 import UCB1\n",
    "from algorithms.modular.moduleUsers.linUCB import LinUCB\n",
    "from algorithms.modular.moduleUsers.linUCB2 import LinUCB2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tnum_arm = 7\n",
    "\tT = 10000\n",
    "\n",
    "\t# Stochastic\n",
    "\tuser_features = [0.5, 0.3, 0.4, 0.2, 0.9, 0]\n",
    "\tTrial = 8\n",
    "\tnoise = gn(1,0,0.01,[-0.2,0.2])\n",
    "\tenvs = list()\n",
    "\tenvs.append(EnvContextual(num_arm, user_features, noise, False))\n",
    "\tenvs.append(EnvContextual(num_arm, user_features, noise, True))\n",
    "\n",
    "\n",
    "\talgorithms = []\n",
    "\talgorithms.append(LinUCB(T, num_arm, len(user_features), alpha=0.5))\n",
    "\talgorithms.append(LinUCB2(T, num_arm, len(user_features), alpha=0.5))\n",
    "\talgorithms.append(UCB1(T, num_arm, xi=0.5))\n",
    "    \n",
    "\talgorithm_names = []\n",
    "\talgorithm_names.append(\"LinUCB\")\n",
    "\talgorithm_names.append(\"UCB1\")\n",
    "\talgorithm_names.append(\"LinUCB variant\")\n",
    "\tenv_names = [\"contextual\", \"contextual_renewing\"]\n",
    "\n",
    "\tprint(algorithm_names)\n",
    "\ttester.test(T, Trial, envs, algorithms, algorithm_names, env_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df218910-1238-4171-bfe0-1d4c67c728f0",
   "metadata": {},
   "source": [
    "# Non Stationary, Single Objective\n",
    "These tests will compare the performance of different strategies for a MAB with breakpoints (or changepoints), points in time where the expected feedback of one or all arms abruptly changes.\n",
    "\n",
    "## Test 1\n",
    "This test matches the standard UCB algorithm against a sliding window version, which forgets past results as soon as they are farther back in time than the window size, and a Discount version, which diminishes all past results by a set factor every timestep.  \n",
    "They run on a normal environment where the rewards are stable (except for noise) and one with two breakpoints. Note that arm 1 starts with the best reward (0.5) before being overtaken by arm 2 (0.9) and finally returning to the way it was before. Also note that arm 1 is stable despite the breakpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c48aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from environment.env import env_stochastic\n",
    "from environment.envNonStationary import env_non_stationary\n",
    "from algorithms.UCB1 import UCB1\n",
    "from algorithms.discountedUCB import DiscountedUCB\n",
    "from algorithms.slidingWindowUCB import SlidingWindowUCB\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tnum_arm = 3\n",
    "\tT = 10000\n",
    "\n",
    "\t# Stochastic\n",
    "\tmu1 = np.array([0.5, 0.3, 0.4])\n",
    "\tmu2 = np.array([0.5, 0.3, 0.9])\n",
    "\tmu3 = np.array([0.5, 0.3, 0.4])\n",
    "\tTrial = 8\n",
    "\tnoise = gn(1,0,0.01,[-0.2,0.2])\n",
    "\tenvs = list()\n",
    "\tenvs.append(env_stochastic(num_arm, mu1, noise))\n",
    "\tenvs.append(env_non_stationary(num_arm, [mu1, mu2, mu3], noise, [3000, 5000]))\n",
    "\n",
    "\n",
    "\talgorithms = [UCB1(T, num_arm, xi=0.5), DiscountedUCB(T, num_arm, 0.9975, xi=0.5), SlidingWindowUCB(T, num_arm, 800, xi=0.5)]\n",
    "\talgorithm_names = [\"UCB1\", \"DiscountedUCB\", \"SlidingWindowUCB\"]\n",
    "\tenv_names = [\"stochastic\", \"non stationary\"]\n",
    "\n",
    "\tprint(algorithm_names)\n",
    "\ttester.test(T, Trial, envs, algorithms, algorithm_names, env_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1adca",
   "metadata": {},
   "source": [
    "As can be seen, as long as the whole environment stays stable, the UCB-strategy performs best and is also the only one to achieve sub-linear regret. The other two strategies fail at this aspect: Because both of them gradually discard their history in some way, they are forced to pull non-optimal arms on a regular basis in order to renew their history, leading to linear regret.  \n",
    "On the other hand, this is what enables them to react to breakpoints much faster than standard UCB, which keeps pulling arms that are no longer optimal without noticing.  \n",
    "Note that, while Sliding Window and Discount perform the same asymptotically, Sliding Window is slightly better and its performance is more coherent in both environments.\n",
    "\n",
    "## Test 2\n",
    "This test adds forced-exploration-UCB with the breakpoint detector from GLR_klUCB, another with the BOCD breakpoint detector and yet another with the Monitor from Monitored UCB, while dropping Standard UCB and the stable environment, which where already covered in the previous test. Discount is dropped too because it performed almost the same as Sliding window, which has been left in the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d106e-ee64-4f2a-9c46-9c5913274a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.env import env_stochastic\n",
    "from environment.envNonStationary import env_non_stationary\n",
    "from algorithms.UCB1 import UCB1\n",
    "from algorithms.slidingWindowUCB import SlidingWindowUCB\n",
    "from algorithms.monitoredUCB import MonitoredUCB\n",
    "from algorithms.modular.moduleUsers.glr_klUCB import GLR_klUCB\n",
    "from algorithms.modular.moduleUsers.bocd import BOCD\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tnum_arm = 3\n",
    "\tT = 10000\n",
    "\n",
    "\tmu1 = np.array([0.5, 0.3, 0.4])\n",
    "\tmu2 = np.array([0.5, 0.3, 0.9])\n",
    "\tmu3 = np.array([0.5, 0.3, 0.4])\n",
    "\tTrial = 8\n",
    "\tnoise = gn(1,0,0.01,[-0.2,0.2])\n",
    "\tenvs = list()\n",
    "\tenvs.append(env_non_stationary(num_arm, [mu1, mu2, mu3], noise, [3000, 5000]))\n",
    "\n",
    "\t# tuning for\n",
    "\t# T=10000, M=3, v(i+1)-v(i) >= 2000, delta(k,i) = 0.5\n",
    "\t# So 3 < floor(10000/L) and 2000 > L\n",
    "\t# and 0.5 >= 2sqrt(log(2KT^2)/w) + 2sqrt(log(2T)/w)\n",
    "\t# => 0.5 >= 2(2.881 + 2.074)/sqrt(w) => 0.5*sqrt(w) >= 9.91\n",
    "\n",
    "\t# One solution: w = 50, L=50*ceil(3/gamma)\n",
    "\t# gamma=0.1\n",
    "\t#406,0.65\n",
    "\n",
    "\talgorithms = list()\n",
    "\talgorithms.append(SlidingWindowUCB(T, num_arm, 800, xi=0.5))\n",
    "\talgorithms.append(MonitoredUCB(T, num_arm, w=50, b=3, gamma=0.1))\n",
    "\talgorithms.append(GLR_klUCB(T, num_arm, alpha=0.03, delta=0.01, global_restart=True, lazyness=10))\n",
    "\talgorithms.append(GLR_klUCB(T, num_arm, alpha=0.03, delta=0.01, global_restart=False, lazyness=10))\n",
    "\talgorithms.append(BOCD(T, num_arm, alpha=0.03))\n",
    "\n",
    "\talgorithm_names = [\"SlidingWindowUCB\", \"MonitoredUCB\", \"GLR-klUCB glob\", \"GLR-klUCB loc\", \"BOCD\"]\n",
    "\tenv_names = [\"non stationary\"]\n",
    "\n",
    "\ttester.test(T, Trial, envs, algorithms, algorithm_names, env_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead60f1-fafe-42be-ade7-667f6b80019d",
   "metadata": {},
   "source": [
    "Again, we get coherent performance from the passive breakpoint adaptor, the Sliding Window, though this time, it is eventually outperformed by the active adaptors when there are no more breakpoints.  \n",
    "The Monitor outperforms the more sophisticated GLR detector, but arguably because the breakpoints here are easy to notice. Due to GLR testing all possible window segmentations, it may be more reliably in settings not covered here.  \n",
    "Speaking of GLR, the local reset version (that only resets the arm in which the breakpoint was detected) notably outperforms the global reset version (that resets everything). However, this comes with a high cost: The global version is already about 10 times slower than the other adaptors and the locals version is 2 times than the global still, and this is already when skipping 9 out of 10 calls (lazyness). The local version is slower because, when less data is reset, there naturally is more data left to compute on.  \n",
    "Asserting the perfomance of BOCD is less straight forward. It exhibits an extensive variation and thus poor reliability in one-shot experiments, and it is neither the best-performing nor the most time-efficient approach. On the other hand, works without any parameters at all without being significantly worse than adaptors that require much more fine tuning for the setting, the details of which might not be known beforehand.\n",
    "\n",
    "## Monitor efficency test\n",
    "This test is to compare 2 versions of Monitored UCB: One that faithfully implements the original algorithm and sums up all numbers in the 2 windows every time a check takes place, and another that saves the sum and only adds the new and substract the new value, cutting time consumption for the check from O(w) to O(1), at the cost of possibly losing accuracy due to limited floating point precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a54b5-1db8-4179-9d10-949f4eb7a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.envNonStationary import env_non_stationary\n",
    "from algorithms.monitoredUCB import MonitoredUCB\n",
    "from algorithms.monitoredUCBOriginal import MonitoredUCBOriginal\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tnum_arm = 3\n",
    "\tT = 10000\n",
    "\n",
    "\tmu1 = np.array([0.5, 0.3, 0.4])\n",
    "\tmu2 = np.array([0.5, 0.3, 0.9])\n",
    "\tmu3 = np.array([0.5, 0.3, 0.4])\n",
    "\tTrial = 4\n",
    "\tnoise = gn(1,0,0.01,[-0.2,0.2])\n",
    "\tenvs = list()\n",
    "\tenvs.append(env_non_stationary(num_arm, [mu1, mu2, mu3], noise, [3000, 5000]))\n",
    "\n",
    "\talgorithms = [MonitoredUCB(T, num_arm, w=50, b=3, gamma=0.1), MonitoredUCBOriginal(T, num_arm, w=50, b=3, gamma=0.1)]\n",
    "\talgorithm_names = [\"MonitoredUCB\", \"MonitoredUCBOriginal\"]\n",
    "\tenv_names = [\"non stationary\"]\n",
    "\n",
    "\ttester.test(T, Trial, envs, algorithms, algorithm_names, env_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf4677e-8662-43f5-9054-f6d6ed96635f",
   "metadata": {},
   "source": [
    "As can be seen, the simplified version basically performs the same, but requires only about 3/4 of the time. This means there is no reason not to use it, which was already done in test 1 and test 2.\n",
    "\n",
    "## Modular Monitor verification\n",
    "Our first implementation of Monitored UCB monolithically implemented the whole strategy, but we switched to modular approaches later on. This test is simply to ensure there was no mistake in the transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83c470-51ae-4bdd-bd45-6aa589efe565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.envNonStationary import env_non_stationary\n",
    "from algorithms.monitoredUCB import MonitoredUCB\n",
    "from algorithms.modular.moduleUsers.monitoredUCB import MonitoredUCB as MonitoredUCBModular\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tnum_arm = 3\n",
    "\tT = 10000\n",
    "\n",
    "\tmu1 = np.array([0.5, 0.3, 0.4])\n",
    "\tmu2 = np.array([0.5, 0.3, 0.9])\n",
    "\tmu3 = np.array([0.5, 0.3, 0.4])\n",
    "\tTrial = 10\n",
    "\tnoise = gn(1,0,0.01,[-0.2,0.2])\n",
    "\tenvs = list()\n",
    "\tenvs.append(env_non_stationary(num_arm, [mu1, mu2, mu3], noise, [3000, 5000]))\n",
    "\n",
    "\n",
    "\talgorithms = list()\n",
    "\talgorithms.append(MonitoredUCB(T, num_arm, w=50, b=3, gamma=0.1))\n",
    "\talgorithms.append(MonitoredUCBModular(T, num_arm, w=50, b=3, gamma=0.1))\n",
    "\n",
    "\talgorithm_names = [\"Old\", \"Modular\"]\n",
    "\tenv_names = [\"non stationary\"]\n",
    "\n",
    "\ttester.test(T, Trial, envs, algorithms, algorithm_names, env_names)\n",
    "\t# Basically the same results, but:\n",
    "\t#Average time for Old: 1.102609669999947 seconds.\n",
    "\t#Average time for Modular: 1.1457539999999427 seconds.\n",
    "\n",
    "\t# Open issue, low priority: Why is the modular version notably slower? (Object-orientation overhead or something avoidable?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6bbbf8-e0ab-4e6b-ba28-c5fc7c31d99c",
   "metadata": {},
   "source": [
    "# Continuous Optimization\n",
    "The following tests test our implementations for strategies for continuous optimization, that is, a possibly unknown, possibly changing function that accepts a multi-dimensional input and returns a cost that we want to minimize on the long term.\n",
    "\n",
    "## Test 1\n",
    "The first test compares the performance of Greedy Projection and of BGD on 3 different settings: One where the cost function stays the same, one where it sometimes shifts to another cost function (with the optimal points being distributed around a non-changin center) and one where it shifts after every timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3a250-8ad6-43f1-83ad-fcde3330dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.envParabola import EnvParabola\n",
    "from algorithms.greedyProjection import GreedyProjection\n",
    "from algorithms.bgd import BGD\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tT = 100\n",
    "\trepeats = 10\n",
    "\n",
    "\tenvs = list()\n",
    "\tenvs.append(EnvParabola(dimensions=3, pos_mean=0, pos_scale=5, slope_mean=1, slope_scale=0.5, boundaries=10, stability=1))\n",
    "\tenvs.append(EnvParabola(dimensions=3, pos_mean=0, pos_scale=5, slope_mean=1, slope_scale=0.5, boundaries=10, stability=0.3))\n",
    "\tenvs.append(EnvParabola(dimensions=3, pos_mean=0, pos_scale=5, slope_mean=1, slope_scale=0.5, boundaries=10, stability=0))\n",
    "\n",
    "\n",
    "\talgorithms = [GreedyProjection(T), BGD(T)]\n",
    "\talgorithm_names = [\"Greedy Projection\", \"BGD\"]\n",
    "\tenv_names = [\"fully stable\", \"less stable\", \"unstable\"]\n",
    "\n",
    "\ttester.test(T, repeats, envs, algorithms, algorithm_names, env_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5176d-5e5e-4554-be1d-b86961e3a86c",
   "metadata": {},
   "source": [
    "The fully stable environment obviously is the optimal setting for Greedy Projection: It quickly gets close to the optimal costs and then only needs to stay there. Note that in all other cases, BGD can keep up with it, which is impressive because it never sees the cost functions and only operates on the feedback. In the unstable setting, it even manages to outperform Greedy Projection.  \n",
    "Interestingly, in the less stable setting, Greedy Projection still performs best, though not by much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883510a9-c5dc-401d-b66e-dff44c734777",
   "metadata": {},
   "source": [
    "## Test 2\n",
    "In the second test, instability is deterministic: A stability of 0.99 (shift in 0.01 of cases) means that a shift occurs after _exactly_ every 100 timesteps.  \n",
    "This time, we match up the previous BGD against its experts version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524cd77d-e859-44e0-bc71-cd847ec3f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.envParabola import EnvParabola\n",
    "from algorithms.bgd import BGD\n",
    "from algorithms.modular.metaPBGD import MetaPBGD\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tT = 1000\n",
    "\trepeats = 10\n",
    "\n",
    "\tenvs = []\n",
    "\tenvs.append(EnvParabola(dimensions=3, pos_mean=0, pos_scale=5, slope_mean=1, slope_scale=0.5, boundaries=10, stability=0.99, fixed_breakpoints=True))\n",
    "\tenvs.append(EnvParabola(dimensions=3, pos_mean=0, pos_scale=5, slope_mean=1, slope_scale=0.5, boundaries=10, stability=1))\n",
    "\n",
    "\n",
    "\talgorithms = []\n",
    "\talgorithms.append(MetaPBGD(T))\n",
    "\talgorithms.append(BGD(T))\n",
    "\talgorithm_names = [\"MetaBGD\", \"BGD\"]\n",
    "\tenv_names = [\"almost stable\", \"fully stable\"]\n",
    "\n",
    "\ttester.test(T, repeats, envs, algorithms, algorithm_names, env_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38644ae-2962-4a46-977a-7d01193b1f8c",
   "metadata": {},
   "source": [
    "## LTC Test\n",
    "The last test tests on environment with not only boundaries, but also constraints. These constraints can be violated, but should be kept on average. TODO: Fix the BGD version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c805f8e8-8f7e-4775-9f45-1a8107b61fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.envParabola import EnvParabola\n",
    "from algorithms.bgd import BGD\n",
    "from algorithms.gb_oco_ltc import GB_OCO_LTC\n",
    "from algorithms.gb_oco_ltc_bgd import GB_OCO_LTC_BGD\n",
    "#from algorithms.modular.metaPBGD import MetaPBGD\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tT = 10000\n",
    "\trepeats = 8\n",
    "\t\n",
    "\tconstrainted_env = EnvParabola(dimensions=3, pos_mean=0.5, pos_scale=0.2, slope_mean=1, slope_scale=0, boundaries=10, stability=1)\n",
    "\tvars = constrainted_env.getVariables()\n",
    "\tconstraint1 = vars[0] + vars[1] + vars[2] - 1\n",
    "\tconstrainted_env.addConstraint(constraint1)\n",
    "\t\n",
    "\tenvs = []\n",
    "\tenvs.append(constrainted_env)\n",
    "\t\n",
    "\t\n",
    "\talgorithms = []\n",
    "\t#algorithms.append(MetaPBGD(T))\n",
    "\talgorithms.append(GB_OCO_LTC(T, 0.05, 1))\n",
    "\talgorithms.append(GB_OCO_LTC_BGD(T, 0.05, 1))\n",
    "\talgorithms.append(GB_OCO_LTC_BGD(T, 1000, 1))\n",
    "\talgorithm_names = [\"Analytical\", \"Estimated\", \"Estimated_constraint_overdrive\"]\n",
    "\tenv_names = [\"env\"]\n",
    "\t\n",
    "\ttester.test(T, repeats, envs, algorithms, algorithm_names, env_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccda830-303b-407f-96c6-8fa4a4e2d7fd",
   "metadata": {},
   "source": [
    "# Multi Objective\n",
    "The following tests return to the MAB, but with multi-objective, that is, the costs are multi-dimensional and each dimensions represents a different objective that shall not be ignored. Most strategies consider the Gini Regret, but we also included the Nash Regret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d050f19-76dd-41a8-b7ec-84a6ea442098",
   "metadata": {},
   "source": [
    "## Basic Test 1\n",
    "This only tests the MO OGDE strategy on a multi-objective environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a668849-ea99-4253-a37a-9b2c0bf1e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.Gaussian_noise import Gaussian_noise as gn\n",
    "from environment.multiOutput import EnvMultiOutput\n",
    "from algorithms.modular.moduleUsers.basicMultiObjective import BasicMultiObjective\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tnum_arm = 3\n",
    "\tT = 200\n",
    "\n",
    "\tweights = [1, 1/2]\n",
    "\n",
    "\tmu = []\n",
    "\tmu.append(np.array([0.1, 0.3]))\n",
    "\tmu.append(np.array([0.2, 0.1]))\n",
    "\tmu.append(np.array([0.1, 0.4]))\n",
    "\n",
    "\ttrial = 10\n",
    "\tnoise = gn(1,0,0.01,[-0.2,0.2])\n",
    "\tenvs = list()\n",
    "\tenvs.append(EnvMultiOutput(num_arm, mu, noise, weights))\n",
    "\n",
    "\talgorithms = list()\n",
    "\talgorithms.append(BasicMultiObjective(T, num_arm, num_objectives=2, delta=0.95, gini_weights=weights))\n",
    "\n",
    "\talgorithm_names = [\"MO_OGDE\"]\n",
    "\tenv_names = [\"Simple\"]\n",
    "\n",
    "\ttester.test(T, trial, envs, algorithms, algorithm_names, env_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5785e4c7-0ed4-413e-b7ac-c609f1d404c6",
   "metadata": {},
   "source": [
    "## Test Original 1\n",
    "This tests tries to replicate the results from the original MO-OGDE paper to assert that our implementation works as intended. It will run the strategy on ~100~ 64 different, randomly generated environments for ~10^5~ (10^4)*5 timesteps. (The environments are deepcopies from one root environment, so we set refresh_first to true to perform a re-init after the deepcopy so the randomness is really there.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5cee56-3b84-4c3e-824f-162d9cfb0736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.multiOutputRandomized import EnvMultiOutputRandomized\n",
    "from algorithms.modular.moduleUsers.basicMultiObjective import BasicMultiObjective\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tnum_arm = 5\n",
    "\tnum_objectives=5\n",
    "\n",
    "\tT = (10**4)*5\n",
    "\n",
    "\tweights = []\n",
    "\tfor i in range(0, 20):\n",
    "\t\tweights.append(1/(2**i))\n",
    "\n",
    "\ttrial = 64\n",
    "\tenvs = list()\n",
    "\tenvs.append(EnvMultiOutputRandomized(num_arm, num_objectives, weights))\n",
    "\n",
    "\talgorithms = list()\n",
    "\talgorithms.append(BasicMultiObjective(T, num_arm=num_arm, num_objectives=num_objectives, delta=0.05, gini_weights=weights))\n",
    "\n",
    "\talgorithm_names = [\"MO_OGDE\"]\n",
    "\tenv_names = [\"random environment\"]\n",
    "\n",
    "\ttester.test(T, trial, envs, algorithms, algorithm_names, env_names, logscale=True, refresh_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec6be4-9415-403c-914f-f38558c160a0",
   "metadata": {},
   "source": [
    "## Test Original 2\n",
    "This test is similar to the one above, but this time, its objective is to look whether dropping a learning rate aspect of the strategy that is required for theoretical guarantees has any impact when running it.\n",
    "\n",
    "Additionally, we further reduced the complexity of the test to make it finish within a more reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d853f-a27f-4515-a811-f82f8c0953b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.multiOutputRandomized import EnvMultiOutputRandomized\n",
    "from algorithms.modular.moduleUsers.basicMultiObjective import BasicMultiObjective\n",
    "from algorithms.modular.moduleUsers.deltalessMultiObjective import DeltalessMultiObjective\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tnum_arm = 5\n",
    "\tnum_objectives=5\n",
    "\n",
    "\tT = 10**4\n",
    "\n",
    "\tweights = []\n",
    "\tfor i in range(0, 20):\n",
    "\t\tweights.append(1/(2**i))\n",
    "\n",
    "\ttrial = 64\n",
    "\tenvs = list()\n",
    "\tenvs.append(EnvMultiOutputRandomized(num_arm, num_objectives, weights))\n",
    "\n",
    "\talgorithms = list()\n",
    "\talgorithms.append(DeltalessMultiObjective(T, num_arm=num_arm, num_objectives=num_objectives, gini_weights=weights))\n",
    "\talgorithms.append(BasicMultiObjective(T, num_arm=num_arm, num_objectives=num_objectives, delta=0.05, gini_weights=weights))\n",
    "\n",
    "\talgorithm_names = [\"Deltaless MO_OGDE\", \"MO_OGDE\"]\n",
    "\tenv_names = [\"random environment\"]\n",
    "\n",
    "\ttester.test(T, trial, envs, algorithms, algorithm_names, env_names, logscale=True, refresh_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01688d6a-d5be-423f-9948-d679efee6dff",
   "metadata": {},
   "source": [
    "The deltaless version is at the very least on par with the original version, at least from a practical point of view. So in the end, we did not break the strategy, which allowed us to further follow this approach.\n",
    "\n",
    "## Test Original 3\n",
    "Previously, we managed to free the strategy from its special learning rate parameter without measurable impact on performance. This allowed us in the next step to instead include a parameter that directly controls the learning rate and build a meta learning strategy with experts from that. This test is to see the differences in performance in the original testcase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2748654a-cfc0-45b6-91dd-3b921746a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.multiOutputRandomized import EnvMultiOutputRandomized\n",
    "from algorithms.modular.moduleUsers.basicMultiObjective import BasicMultiObjective\n",
    "from algorithms.modular.moduleUsers.expertsMultiObjective import ExpertsMultiObjective\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tnum_arm = 5\n",
    "\tnum_objectives=5\n",
    "\n",
    "\tT = 10**4\n",
    "\n",
    "\tweights = []\n",
    "\tfor i in range(0, 20):\n",
    "\t\tweights.append(1/(2**i))\n",
    "\n",
    "\ttrial = 16\n",
    "\tenvs = list()\n",
    "\tenvs.append(EnvMultiOutputRandomized(num_arm, num_objectives, weights))\n",
    "\n",
    "\talgorithms = list()\n",
    "\talgorithms.append(ExpertsMultiObjective(T, num_arm=num_arm, num_objectives=num_objectives, gini_weights=weights))\n",
    "\talgorithms.append(BasicMultiObjective(T, num_arm=num_arm, num_objectives=num_objectives, delta=0.05, gini_weights=weights))\n",
    "\n",
    "\talgorithm_names = [\"Meta MO_OGDE\", \"MO_OGDE\"]\n",
    "\tenv_names = [\"random environment\"]\n",
    "\n",
    "\ttester.test(T, trial, envs, algorithms, algorithm_names, env_names, logscale=True, refresh_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012f58d-9904-4161-838b-bc59df10401b",
   "metadata": {},
   "source": [
    "By appliying the experts paradigm to the strategy, we managed to improve its performance by a little, at least in practise and for some cases, without worsening it in others. Do note however that this comes wih a considerable impact on the runtime. As long as the environment is stationary, there is no real benefit, but we test the strategy for non-stationary settings later on.\n",
    "\n",
    "## Breakpoint Test 1\n",
    "This test set runs on an environment that is similar to one in basic test 1, but includes breakpoints. As a novel element, the MO OGDE strategy has been combined with some of the breakpoint adaptors originally intended for usage in single objective settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce001a-64c0-4f0c-9210-80f1ea068b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.multiOutput import EnvMultiOutput\n",
    "from environment.multiOutputNonStationary import EnvMultiOutputNonStationary\n",
    "from algorithms.modular.moduleUsers.basicMultiObjective import BasicMultiObjective\n",
    "from algorithms.modular.moduleUsers.slidingWindowMO import SlidingWindowMO\n",
    "from algorithms.modular.moduleUsers.discountedMO import DiscountedMO\n",
    "from algorithms.modular.moduleUsers.monitoredMO import MonitoredMO\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tnum_arm = 3\n",
    "\tT = 200\n",
    "\n",
    "\tweights = [1, 1/2]\n",
    "\n",
    "\tmu1 = []\n",
    "\tmu1.append(np.array([0.1, 0.3]))\n",
    "\tmu1.append(np.array([0.2, 0.1]))\n",
    "\tmu1.append(np.array([0.1, 0.4]))\n",
    "\n",
    "\t\"\"\"mu2 = []\n",
    "\tmu2.append(np.array([0.12, 0.17]))\n",
    "\tmu2.append(np.array([0.15, 0.2]))\n",
    "\tmu2.append(np.array([0.1, 0.4]))\"\"\"\n",
    "\n",
    "\tmu2 = []\n",
    "\tmu2.append(np.array([0.1, 0.3]))\n",
    "\tmu2.append(np.array([0.1, 0.4]))\n",
    "\tmu2.append(np.array([0.2, 0.1]))\n",
    "\n",
    "\tmu3 = []\n",
    "\tmu3.append(np.array([0.1, 0.3]))\n",
    "\tmu3.append(np.array([0.2, 0.1]))\n",
    "\tmu3.append(np.array([0.1, 0.4]))\n",
    "\n",
    "\ttrial = 10\n",
    "\tnoise = gn(1,0,0.01,[-0.2,0.2])\n",
    "\tenvs = list()\n",
    "\tenvs.append(EnvMultiOutput(num_arm, mu1, noise, weights))\n",
    "\tenvs.append(EnvMultiOutputNonStationary(num_arm, [mu1, mu2, mu3], noise, weights, [100, 150]))\n",
    "\n",
    "\talgorithms = list()\n",
    "\talgorithms.append(BasicMultiObjective(T, num_arm, num_objectives=2, delta=0.05, gini_weights=weights))\n",
    "\talgorithms.append(SlidingWindowMO(T, num_arm, num_objectives=2, delta=0.05, gini_weights=weights, window_len=20))\n",
    "\talgorithms.append(DiscountedMO(T, num_arm, num_objectives=2, delta=0.05, gini_weights=weights, gamma=0.95))\n",
    "\talgorithms.append(MonitoredMO(T, num_arm, num_objectives=2, delta=0.05, gini_weights=weights, w=30, b=1))\n",
    "\n",
    "\talgorithm_names = []\n",
    "\talgorithm_names.append(\"MO_OGDE\")\n",
    "\talgorithm_names.append(\"SlidingWindow\")\n",
    "\talgorithm_names.append(\"Discounted\")\n",
    "\talgorithm_names.append(\"Monitored\")\n",
    "\tenv_names = []\n",
    "\tenv_names.append(\"Simple\")\n",
    "\tenv_names.append(\"withBreakpoints\")\n",
    "\n",
    "\ttester.test(T, trial, envs, algorithms, algorithm_names, env_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee907f4-b76d-477e-9333-fa2e2ef4d854",
   "metadata": {},
   "source": [
    "The main takeaway is that the original MO OGDE outperforms the other strategies in the breakpoint-less environment (which is no surprise), but is also the only one to be obviously thrown off by breakpoints in the other environment. (The result presentation will be improved in the next test.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ea214-fad4-471b-8a85-9c504b031aa2",
   "metadata": {},
   "source": [
    "## Breakpoint Test 2\n",
    "In this test, we greatly increase the number of timesteps and the number of repetitions while dropping the breakpoint-less environment, which is not relevant at this point and would overly clutter the result visualization. Instead, we include two more adaptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd17a3-9107-40f1-8c52-e42b7edabbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.multiOutput import EnvMultiOutput\n",
    "from environment.multiOutputNonStationary import EnvMultiOutputNonStationary\n",
    "from algorithms.modular.moduleUsers.basicMultiObjective import BasicMultiObjective\n",
    "from algorithms.modular.moduleUsers.slidingWindowMO import SlidingWindowMO\n",
    "from algorithms.modular.moduleUsers.discountedMO import DiscountedMO\n",
    "from algorithms.modular.moduleUsers.monitoredMO import MonitoredMO\n",
    "from algorithms.modular.moduleUsers.bocdMO import BOCD_MO\n",
    "from algorithms.modular.moduleUsers.glrMO import GLR_MO\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tnum_arm = 3\n",
    "\tT = 10000\n",
    "\n",
    "\tweights = [1, 1/2]\n",
    "\n",
    "\tmu1 = []\n",
    "\tmu1.append(np.array([0.1, 0.3]))\n",
    "\tmu1.append(np.array([0.2, 0.1]))\n",
    "\tmu1.append(np.array([0.1, 0.4]))\n",
    "\n",
    "\tmu2 = []\n",
    "\tmu2.append(np.array([0.1, 0.3]))\n",
    "\tmu2.append(np.array([0.1, 0.4]))\n",
    "\tmu2.append(np.array([0.2, 0.1]))\n",
    "\n",
    "\tmu3 = []\n",
    "\tmu3.append(np.array([0.1, 0.3]))\n",
    "\tmu3.append(np.array([0.2, 0.1]))\n",
    "\tmu3.append(np.array([0.1, 0.4]))\n",
    "\n",
    "\ttrial = 16\n",
    "\tnoise = gn(1,0,0.01,[-0.2,0.2])\n",
    "\tenvs = list()\n",
    "\tenvs.append(EnvMultiOutputNonStationary(num_arm, [mu1, mu2, mu3], noise, weights, [3000, 5000]))\n",
    "\n",
    "\tdelta = 0.05\n",
    "\talgorithms = list()\n",
    "\talgorithms.append(BasicMultiObjective(T, num_arm, num_objectives=2, delta=delta, gini_weights=weights))\n",
    "\talgorithms.append(SlidingWindowMO(T, num_arm, num_objectives=2, delta=delta, gini_weights=weights, window_len=800))\n",
    "\talgorithms.append(DiscountedMO(T, num_arm, num_objectives=2, delta=delta, gini_weights=weights, gamma=0.9975))\n",
    "\talgorithms.append(MonitoredMO(T, num_arm, num_objectives=2, delta=delta, gini_weights=weights, w=50, b=3))\n",
    "\talgorithms.append(BOCD_MO(T, num_arm, num_objectives=2, delta=delta, gini_weights=weights))\n",
    "\talgorithms.append(GLR_MO(T, num_arm, num_objectives=2, delta=delta, delta2=0.01, global_restart=True, lazyness=10, gini_weights=weights))\n",
    "\n",
    "\talgorithm_names = []\n",
    "\talgorithm_names.append(\"MO_OGDE\")\n",
    "\talgorithm_names.append(\"SlidingWindow\")\n",
    "\talgorithm_names.append(\"Discounted\")\n",
    "\talgorithm_names.append(\"Monitored\")\n",
    "\talgorithm_names.append(\"BOCD_MO\")\n",
    "\talgorithm_names.append(\"GLR_MO\")\n",
    "\tenv_names = []\n",
    "\tenv_names.append(\"withBreakpoints\")\n",
    "\n",
    "\ttester.test(T, trial, envs, algorithms, algorithm_names, env_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f846c-7c67-442a-98eb-2a50a5a614ed",
   "metadata": {},
   "source": [
    "The results are clearer in this version. As before, original MO OGDE cannot handle the breakpoints correctly and loses against all its versions that include a breakpoint adaptor.  \n",
    "Like in the single objective version, the Monitor adaptor proves to be highly efficient as long as both the window size and the detection threshold are chosen well.  \n",
    "GLR is on par with the passive adaptors, which might not be enough to justify its runtime overhead, but again, the parameters might not be optimal. Speaking of parameters, BOCD can keep up with the others without any parameters, as before. Interstingly, it seems to perform better in terms of Nash regret and Pareto regret, even though the underlying MO OGDE focuses on the Gini regret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb07ef1-d24b-4ee3-9dce-c54463d0d501",
   "metadata": {},
   "source": [
    "## Pareto Test\n",
    "This test compares the performance of Pareto UCB against that of MO OGDE. Note that the former does not optimize for any regret, but only aims to choose arms from the Pareto front, which may or may not still lead to low regret.  \n",
    "The test runs on two environment, one of which includes bad arms that are still within the Pareto front and thus should be tricky for handle for Pareto UCB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35406665-11ac-4e44-b9a0-40e2d966b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.multiOutput import EnvMultiOutput\n",
    "from algorithms.modular.moduleUsers.paretoUCB import ParetoUCB\n",
    "from algorithms.modular.moduleUsers.basicMultiObjective import BasicMultiObjective\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tnum_arm = 4\n",
    "\tT = 3000\n",
    "\n",
    "\tweights = [1, 1/2]\n",
    "\n",
    "\tmu1 = []\n",
    "\tmu1.append(np.array([0.1, 0.3]))\n",
    "\tmu1.append(np.array([0.2, 0.1]))\n",
    "\tmu1.append(np.array([0.15, 0.4]))\n",
    "\tmu1.append(np.array([0.3, 0.2]))\n",
    "\t\n",
    "\t# This one might be more tricky for pareto based strategies because they cannot drop any arm.\n",
    "\tmu2 = []\n",
    "\tmu2.append(np.array([0.1, 0.3]))\n",
    "\tmu2.append(np.array([0.2, 0.1]))\n",
    "\tmu2.append(np.array([0.08, 0.4]))\n",
    "\tmu2.append(np.array([0.3, 0.08]))\n",
    "\n",
    "\ttrial = 4\n",
    "\tnoise = gn(1,0,0.01,[-0.2,0.2])\n",
    "\tenvs = list()\n",
    "\tenvs.append(EnvMultiOutput(num_arm, mu1, noise, weights))\n",
    "\tenvs.append(EnvMultiOutput(num_arm, mu2, noise, weights))\n",
    "\n",
    "\tdelta = 0.05\n",
    "\talgorithms = list()\n",
    "\talgorithms.append(ParetoUCB(T, num_arm, num_objectives=2, alpha=1, gini_weights=weights))\n",
    "\talgorithms.append(BasicMultiObjective(T, num_arm, num_objectives=2, delta=0.05, gini_weights=weights))\n",
    "\n",
    "\talgorithm_names = []\n",
    "\talgorithm_names.append(\"ParetoUCB\")\n",
    "\talgorithm_names.append(\"MO_OGDE\")\n",
    "\tenv_names = []\n",
    "\tenv_names.append(\"favourable env\")\n",
    "\tenv_names.append(\"tricky env\")\n",
    "\n",
    "\ttester.test(T, trial, envs, algorithms, algorithm_names, env_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeb0428-71ed-457e-a02a-7654c33a94ee",
   "metadata": {},
   "source": [
    "As expected, Pareto UCB clearly fails on the tricky environment. It  has to choose random arms from the full set of arms because it cannot drop any arms that are within the Pareto front. On the more favourable environment, it can roughly keep up with MO OGDE, but is still clearly outmatched by it. Except for runtime advantages, Pareto UCB is not recommendable for environments with many objectives.  \n",
    "(Remember that an arm is not in the Pareto front iff another arm is at least as good in *all* dimensions and better in one, which naturally becomes more unlikely the more dimensions there are.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb17b0-6cf9-4591-b188-05821b525a63",
   "metadata": {},
   "source": [
    "## Basic Test 2\n",
    "This test compares the experts version of MO OGDE against the Fair MO UCB strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee66a644-6b4a-4475-8b77-54b03efbc6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.multiOutput import EnvMultiOutput\n",
    "from algorithms.modular.moduleUsers.expertsMultiObjective import ExpertsMultiObjective\n",
    "from algorithms.modular.moduleUsers.fair_MO_UCB import Fair_MO_UCB\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tnum_arm = 4\n",
    "\tnum_objectives=5\n",
    "\tT = 10000\n",
    "\t\n",
    "\t\n",
    "\tmu = []\n",
    "\tmu.append(np.array([.15, .3, .5, .72, .8]))\n",
    "\tmu.append(np.array([.2, .18, .48, .7, .2]))\n",
    "\tmu.append(np.array([.15, .4, .3, .8, .4]))\n",
    "\tmu.append(np.array([.2, .2, .8, .3, .5]))\n",
    "\t\n",
    "\tweights = []\n",
    "\tfor i in range(0, num_objectives):\n",
    "\t\tweights.append(1/(2**i))\n",
    "\t\n",
    "\tnoise = gn(1,0,0.01,[-0.2,0.2])\n",
    "\ttrial = 8\n",
    "\tenvs = list()\n",
    "\tenvs.append(EnvMultiOutput(num_arm, mu, noise, weights))\n",
    "\n",
    "\talgorithms = []\n",
    "\talgorithms.append(ExpertsMultiObjective(T, num_arm=num_arm, num_objectives=num_objectives, gini_weights=weights))\n",
    "\talgorithms.append(Fair_MO_UCB(T, num_arm=num_arm, num_objectives=num_objectives, delta=0.1, gini_weights=weights))\n",
    "\t\n",
    "\talgorithm_names = []\n",
    "\talgorithm_names.append(\"ExpertsMultiObjective\")\n",
    "\talgorithm_names.append(\"Fair MO UCB\")\n",
    "\tenv_names = [\"Normal Environment\"]\n",
    "\t\n",
    "\ttester.test(T, trial, envs, algorithms, algorithm_names, env_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24983de-23f0-480e-abaf-a9a44b48db04",
   "metadata": {},
   "source": [
    "Fair MO UCB is clearly outmatched in all apects except time efficiency, which is why we no longer consider it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489359d-a69a-46ed-932b-acf23c1279d7",
   "metadata": {},
   "source": [
    "## Test Meta Breakpoints\n",
    "Compare the normal meta strategy against ones that have breakpoint adaption against the meta strategy plus breakpoint adaption. We expected the latter to be the best of both worlds in a non-stationary setting.\n",
    "\n",
    "For the breakpoint adaptors, we choose Discount, which is a well performing passive adaptor, and BOCD, which is an active adaptor that can almost keep up with other active adaptors such as monitor, but conveniently requires no tuning, as mentioned in a previous test.\n",
    "\n",
    "You may wonder why several function from tester are called instead of test. This is equivalent and to show how the process can be controlled more granulary. For example, by omitting purgeResults and some of the tests, you can plot new tests together with the ones you removed without calculating the latter again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cae0e-5e38-43cb-8c81-49d3991df603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.multiOutputNonStationary import EnvMultiOutputNonStationary\n",
    "from algorithms.modular.moduleUsers.basicMultiObjective import BasicMultiObjective\n",
    "from algorithms.modular.moduleUsers.discountedMO import DiscountedMO\n",
    "from algorithms.modular.moduleUsers.monitoredMO import MonitoredMO\n",
    "from algorithms.modular.moduleUsers.bocdMO import BOCD_MO\n",
    "from algorithms.modular.moduleUsers.expertsMultiObjective import ExpertsMultiObjective\n",
    "from algorithms.modular.moduleUsers.discountedExpertsMultiObjective import DiscountedExpertsMultiObjective\n",
    "from algorithms.modular.moduleUsers.bogdExpertsMultiObjective import BOCD_ExpertsMultiObjective\n",
    "\n",
    "\n",
    "# Note that this environment is a little mean, especially in the second half.\n",
    "if __name__ == \"__main__\":\n",
    "\tnum_arm = 3\n",
    "\tnum_objectives = 3\n",
    "\tT = 3000\n",
    "\n",
    "\tweights = [1, 1/2, 1/4]\n",
    "\n",
    "\tmu1 = []\n",
    "\tmu1.append(np.array([0.1, 0.3, 0.2]))\n",
    "\tmu1.append(np.array([0.2, 0.1, 0.2]))\n",
    "\tmu1.append(np.array([0.1, 0.4, 0.3]))\n",
    "\n",
    "\tmu2 = []\n",
    "\tmu2.append(np.array([0.1, 0.3, 0.1]))\n",
    "\tmu2.append(np.array([0.1, 0.4, 0.2]))\n",
    "\tmu2.append(np.array([0.2, 0.1, 0.1]))\n",
    "\n",
    "\tmu3 = []\n",
    "\tmu3.append(np.array([0.2, 0.1, 0.2]))\n",
    "\tmu3.append(np.array([0.1, 0.4, 0.3]))\n",
    "\tmu3.append(np.array([0.1, 0.3, 0.2]))\n",
    "\n",
    "\tmu4 = []\n",
    "\tmu4.append(np.array([0.1, 0.3, 0.2]))\n",
    "\tmu4.append(np.array([0.2, 0.1, 0.2]))\n",
    "\tmu4.append(np.array([0.1, 0.4, 0.3]))\n",
    "\n",
    "\tmu5 = []\n",
    "\tmu5.append(np.array([0.1, 0.3, 0.2]))\n",
    "\tmu5.append(np.array([0.2, 0.1, 0.2]))\n",
    "\tmu5.append(np.array([0.1, 0.4, 0.3]))\n",
    "\n",
    "\ttrial = 16\n",
    "\tnoise = gn(1,0,0.01,[-0.2,0.2])\n",
    "\tenvs = list()\n",
    "\tenvs.append(EnvMultiOutputNonStationary(num_arm, [mu1, mu2, mu3, mu4, mu5], noise, weights, [700, 1500, 1700, 2300]))\n",
    "\n",
    "\tdelta = 0.05\n",
    "\talgorithms = list()\n",
    "\talgorithms.append(DiscountedMO(T, num_arm, num_objectives=num_objectives, delta=delta, gini_weights=weights, gamma=0.99))\n",
    "\t#algorithms.append(MonitoredMO(T, num_arm, num_objectives=num_objectives, delta=delta, gini_weights=weights, w=50, b=3))\n",
    "\talgorithms.append(BOCD_MO(T, num_arm, num_objectives=num_objectives, delta=delta, gini_weights=weights))\n",
    "\talgorithms.append(BasicMultiObjective(T, num_arm=num_arm, num_objectives=num_objectives, delta=0.05, gini_weights=weights))\n",
    "\talgorithms.append(ExpertsMultiObjective(T, num_arm=num_arm, num_objectives=num_objectives, gini_weights=weights))\n",
    "\talgorithms.append(DiscountedExpertsMultiObjective(T, num_arm=num_arm, num_objectives=num_objectives, gini_weights=weights, gamma=0.99))\n",
    "\talgorithms.append(BOCD_ExpertsMultiObjective(T, num_arm=num_arm, num_objectives=num_objectives, gini_weights=weights))\n",
    "\n",
    "\talgorithm_names = []\n",
    "\talgorithm_names.append(\"Discounted\")\n",
    "\t#algorithm_names.append(\"Monitored\")\n",
    "\talgorithm_names.append(\"BOCD\")\n",
    "\talgorithm_names.append(\"Original\")\n",
    "\talgorithm_names.append(\"Meta\")\n",
    "\talgorithm_names.append(\"Discounted_Meta\")\n",
    "\talgorithm_names.append(\"BOGD_Meta\")\n",
    "\tenv_names = []\n",
    "\tenv_names.append(\"Breakpoint env\")\n",
    "\t\n",
    "\ttester.resultpath = \"metaBreakpointsResults/\"\n",
    "\t\n",
    "\ttester.purgeResults()\n",
    "\ttester.testOnly(T, trial, envs, algorithms, algorithm_names, env_names)\n",
    "\ttester.readAllResults()\n",
    "\ttester.plotResults()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afff46f-7cad-47e9-9042-40627759e3c1",
   "metadata": {},
   "source": [
    "First we regard the strategies without any breakpoint adaptors, which are the original MO-UCB and the Meta-MO-UCB. We observe two very interesting results: On the one hand, both of them take about equally heavy hits to their performance from the breakpoints. However, the meta version shows much better recovery from the second breakpoint, which becomes most obvious from around timestep 2000 onwards. Obviously, having multiple experts with different learning rates and being able to dynamically shift to the best performing one indeed works out as promised.\n",
    "\n",
    "Our other focus was whether or not using the meta version instead of the original version further improves the performance of anything that includes a breakpoint adaptor. (Note how any breakpoint adaption makes a strategy outperform the two strategies without, by a large margin.) Here, the results are less unidirectional and pairing the meta strategy together with the BOCD breakpoint adaptor actually worsened performance by a little. A *possible* explanation is the active adaptor resets *all* experts when it believes there has been a breakpoint, dropping the benefit of having multiple experts that can build vastly different opinions over time.\n",
    "\n",
    "Finally however, when paring the meta version with the Discount breakpoint adaptor, we see the results we expected. Not only does it perform much better than the original version with the Discount, it is on pair or even better (depending on what regret definitions you consider) than both BOCD versions - which can be called impressive for a strategy that only uses passive breakpoint adaption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2864d-a2fa-4b11-95de-3b467de33933",
   "metadata": {},
   "source": [
    "# Operating on real data\n",
    "In this section, we take an sample from the last.fm database and transform it into a MO-MAB-instance with choosable arms and multi-dimensional rewards.\n",
    "\n",
    "## Processing the data\n",
    "Please execute readFM.py to generate the bandit data from a last.fm file. It will grab the most active users and the most popular tracks and generate the expected costs per track (arm) so that they are lower the more a user (dimension / objective) has listened to it. Optionally, set use_correction to true to also consider whether a user is even interested in the popular tracks, which will lead to more realistic responses but generally higher costs that do not differ as much.\n",
    "\n",
    "Four files will be generated: One that considers the full dataset and three that have it arbitrarily divided into daytimes (but still consider the same users and tracks as in the full set) to simulate breakpoints.\n",
    "\n",
    "Finally, note that this approach simulates an online learning problem from offline data to test how a strategy might cope with live data that gradually comes in, which may not necessarily be what you want.\n",
    "\n",
    "## Test FM\n",
    "This test will read in the bandit settings generated in the previous step. It will generate three environments: One where the feedback is the exact costs, one where it is either 0 or 1, but with the probality of the costs, and one that uses the dayphase file instead of the general file to generate a non-stationary setting.\n",
    "\n",
    "All these problems can be imagined as having to generate a single playlist for all users that shall maximize the general satisfaction and that you can modify on the fly. A real world scenario where this applies would be making some good selection of tracks for a radio broadcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896cbe1-5e43-4194-ad92-9c96344fd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.multiOutput import EnvMultiOutput\n",
    "from environment.multiOutputBernulli import EnvMultiOutputBernulli\n",
    "from environment.multiOutputNonStationary import EnvMultiOutputNonStationary\n",
    "from algorithms.modular.moduleUsers.basicMultiObjective import BasicMultiObjective\n",
    "from algorithms.modular.moduleUsers.expertsMultiObjective import ExpertsMultiObjective\n",
    "from algorithms.modular.moduleUsers.paretoUCB import ParetoUCB\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmu = [0]*4\n",
    "\tcalls = [0]*4\n",
    "\tfor dayphase in range(4):\n",
    "\t\tif dayphase == 0:\n",
    "\t\t\tfilename = \"../fm_banditified.csv\"\n",
    "\t\telif dayphase == 1:\n",
    "\t\t\tfilename = \"../fm_banditified_day.csv\"\n",
    "\t\telif dayphase == 2:\n",
    "\t\t\tfilename = \"../fm_banditified_evening.csv\"\n",
    "\t\telse:\n",
    "\t\t\tfilename = \"../fm_banditified_night.csv\"\n",
    "\t\t\n",
    "\t\tmu[dayphase] = np.loadtxt(filename, delimiter=',', encoding=\"utf-8\")\n",
    "\t\ttopstring = \"\"\n",
    "\t\twith open(filename, encoding=\"utf-8\") as file:\n",
    "\t\t\tfor line in file:\n",
    "\t\t\t\tif line[0] == '#':\n",
    "\t\t\t\t\ttopstring += line[1:]\n",
    "\t\t\t\t\tif \"calls=\" in line:\n",
    "\t\t\t\t\t\tcalls[dayphase] = int(line.strip().split('=')[1])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# We expect the comment at the top, and only at the top.\n",
    "\t\t\t\t\tbreak\n",
    "\t\tprint(topstring)\n",
    "\t\t#print(mu[dayphase])\n",
    "\tassert calls[0] == sum(calls[1:]), \"calls in dayphases do not sum up to \"+str(calls[0])\n",
    "\t\n",
    "\tnum_arm = len(mu[0])\n",
    "\tnum_objectives = len(mu[0][0])\n",
    "\tT = 3000\n",
    "\t\n",
    "\tfactor = T/calls[0]\n",
    "\tbreakpoint1 = round((calls[1] + 1)*factor)\n",
    "\tbreakpoint2 = round((calls[1] + calls[2]+1)*factor)\n",
    "\t#print(\"Breakpoints:\", breakpoint1, breakpoint2)\n",
    "\t\n",
    "\tweights = [0]*num_objectives\n",
    "\tfor i in range(num_objectives):\n",
    "\t\tweights[i] = (1/2)**i\n",
    "\n",
    "\n",
    "\ttrial = 4\n",
    "\tnoise = gn(1,0,0.01,[-0.2,0.2])\n",
    "\tenvs = list()\n",
    "\tenvs.append(EnvMultiOutput(num_arm, mu[0], noise, weights))\n",
    "\tenvs.append(EnvMultiOutputBernulli(num_arm, mu[0], weights))\n",
    "\tenvs.append(EnvMultiOutputNonStationary(num_arm, mu[1:], noise, weights, [breakpoint1, breakpoint2]))\n",
    "\n",
    "\tdelta = 0.05\n",
    "\talgorithms = list()\n",
    "\talgorithms.append(ParetoUCB(T, num_arm, num_objectives=num_objectives, alpha=1, gini_weights=weights))\n",
    "\t#algorithms.append(BasicMultiObjective(T, num_arm, num_objectives=num_objectives, delta=0.05, gini_weights=weights))\n",
    "\talgorithms.append(ExpertsMultiObjective(T, num_arm, num_objectives=num_objectives, gini_weights=weights))\n",
    "\n",
    "\talgorithm_names = []\n",
    "\talgorithm_names.append(\"ParetoUCB\")\n",
    "\t#algorithm_names.append(\"MO_OGDE\")\n",
    "\talgorithm_names.append(\"Meta_MO_OGDE\")\n",
    "\tenv_names = []\n",
    "\tenv_names.append(\"transparent_env\")\n",
    "\tenv_names.append(\"bernoulli_env\")\n",
    "\tenv_names.append(\"transparent_dayphase_env\")\n",
    "\n",
    "\ttester.test(T, trial, envs, algorithms, algorithm_names, env_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f07b8-3053-4ed6-9bca-105d92db2082",
   "metadata": {},
   "source": [
    "As could be expected, both strategies struggle more on the dayphase setting. However, they already do so even before the first breakpoint. A possible explanation would be that the set of popular tracks includes some tracks that were not popular at all during certain dayphases, making exploration (every strategy has to perform some exploration early on) much more punishing than in the full setting. Aside from that, ParetoUCB is more successfull with breakpoint handling even without including some explicit breakpoint adaption.\n",
    "\n",
    "Another surprise is that the strategies show little difference when comparing the transparent version with the bernoulli version. In other words, high precision data hardly has any benefit over binary data. And the latter might be easier measured in the real world, for example by simply looking whether the user skips a suggested track at a given time or chooses to listen to it and directly plugging this information into the strategy without any further processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
